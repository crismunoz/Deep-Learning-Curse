{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleRNN - Geração de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN tem sido usado extensamente pela comunidade de Processamento de Linguagem Natural (NLP) para várias aplicações. Uma de elas, por exemplo, é construção de modelos de linguagem. Os modelos de linguagem ajudam a prever a probabilidade da seguinte palavra em função da palavra anterior. Os modelos de linguagem são importantes para várias tarefas de alto nível como máquinas de tradução, correção da pronúncia, etc.\n",
    "\n",
    "# \\begin{equation}\n",
    "h_t = \\phi(h_{t-1} ,x_t)\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"img/RNN-unrolled.png \" width=\"600\">\n",
    "\n",
    "A ideia nesta pratica é a mesma que modelamento baseado em linguagem só que trabalhamos com carateres em ves de palavras assim a velocidade de processamento é melhor.  Primeiro importamos os modulos necessarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla C2070 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lemos o texto de entrada do texto Alice in Wonderland no site [Project Gutenberg ](http://www.gutenberg.org/ebooks/11).  O texto contém linhas de caracteres non-ASCII, pelo qual primeiro limpamos e escrevemos o conteúdo numa variável chamada *text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DL_DATA=\"/share/apps/DL_DATA/LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fin = open(os.path.join(DL_DATA,\"11-0.txt\"),'rb')\n",
    "lines=[]\n",
    "for line in fin:\n",
    "    line = line.strip().lower()\n",
    "    line = line.decode(\"ascii\",\"ignore\")\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O seguinte código mapea os caracteres em índices e vice-versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "char2index = dict((c,i) for i, c in enumerate(chars))\n",
    "index2char = dict((i,c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O seguinte passo é criar os textos de entradas e rótulos. Realizamos isto passando através to texto com um passo STEP e extraímos uma extensão de texto determinada pela variável SEQLEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEQLEN=#10\n",
    "STEP=#1\n",
    "\n",
    "input_chars=[]\n",
    "label_chars=[]\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos vetorizar estes textos de entradas e rótulos. Nossa saída é um único caráter que é representada por um vector one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "Y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "for i, input_char in enumerate (input_chars):\n",
    "    for j,ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    Y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RRN é conectada a uma camada full connected (FC). A camada FC tem (nb_char) units, que calcula valores de probabilidade para cada palavra. O caráter com a maior probabilidade é escolhido como a predição. Para compilar o modelo, utilizamos a função de perda: cross-entropy e o método RMSprop para a atualização de pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = #128\n",
    "BATCH_SIZE = #128\n",
    "NUM_ITERATIONS = #25\n",
    "NUM_EPOCH_PER_ITERATION = #1\n",
    "NUM_PREDS_PER_EPOCH = #100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(HIDDEN_SIZE,return_sequences=False, input_shape=(SEQLEN, nb_chars), unroll=True))\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nosso teste consiste em gerar caracteres desde o modelo dada uma entrada aleatória, logo tirar o primeiro entrada e anexar a saída precedida pela rede. Continuamos o processo 100 vezes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Generation from seed: in a tone \n",
      "in a tone the sard the sald the sald the sald the sald the sald the sald the sald the sald the sald the sald t==================================================\n",
      "Iteration #: 1\n",
      "Generation from seed: s in it? s\n",
      "s in it? said the said the said the said the said the said the said the said the said the said the said the sa==================================================\n",
      "Iteration #: 2\n",
      "Generation from seed:  reasonabl\n",
      " reasonabling to see for and the gryphon and the gryphon and the gryphon and the gryphon and the gryphon and t==================================================\n",
      "Iteration #: 3\n",
      "Generation from seed:  to come u\n",
      " to come unded to the reat the beras of the was the was the was the was the was the was the was the was the wa==================================================\n",
      "Iteration #: 4\n",
      "Generation from seed: site which\n",
      "site which said the mare the project gutenberg-tm alice found the dormouse of the gropen a dither the reat one==================================================\n",
      "Iteration #: 5\n",
      "Generation from seed: answer que\n",
      "answer queen said the mouse of the had the forme said the mouse of the had the forme said the mouse of the had==================================================\n",
      "Iteration #: 6\n",
      "Generation from seed:  owner of \n",
      " owner of the mouse the mouse the mouse the mouse the mouse the mouse the mouse the mouse the mouse the mouse ==================================================\n",
      "Iteration #: 7\n",
      "Generation from seed: es where w\n",
      "es where was she was she was she was she was she was she was she was she was she was she was she was she was s==================================================\n",
      "Iteration #: 8\n",
      "Generation from seed: le door in\n",
      "le door in the rabbit was the march hare the march hare the march hare the march hare the march hare the march==================================================\n",
      "Iteration #: 9\n",
      "Generation from seed: ood near t\n",
      "ood near the mock turtle said the mock turtle said the mock turtle said the mock turtle said the mock turtle s==================================================\n",
      "Iteration #: 10\n",
      "Generation from seed: r, you kno\n",
      "r, you know the mouse to the project gutenberg-tm electronic works a little thing the mouse to the project gut==================================================\n",
      "Iteration #: 11\n",
      "Generation from seed: can really\n",
      "can really again the gryphon and the king said the gryphon and the king said the gryphon and the king said the==================================================\n",
      "Iteration #: 12\n",
      "Generation from seed: ng. the ne\n",
      "ng. the next of the more the more the more the more the more the more the more the more the more the more the ==================================================\n",
      "Iteration #: 13\n",
      "Generation from seed: ch, said a\n",
      "ch, said alice was go and alice was go and alice was go and alice was go and alice was go and alice was go and==================================================\n",
      "Iteration #: 14\n",
      "Generation from seed: then it ou\n",
      "then it out of the morely again, and she had the mores a dont that she had she had the mores a dont that she h==================================================\n",
      "Iteration #: 15\n",
      "Generation from seed: ite right,\n",
      "ite right, said the king said the king said the king said the king said the king said the king said the king s==================================================\n",
      "Iteration #: 16\n",
      "Generation from seed: -tm electr\n",
      "-tm electronic works and alice would not me one of the some the project gutenberg-tm electronic works and alic==================================================\n",
      "Iteration #: 17\n",
      "Generation from seed: the garden\n",
      "the garden a look of the mock turtle see who had a long the mock turtle see who had a long the mock turtle see==================================================\n",
      "Iteration #: 18\n",
      "Generation from seed: e--but im \n",
      "e--but im a think i should be a little share she was the dormouse in a long the caterpillar she had the door, ==================================================\n",
      "Iteration #: 19\n",
      "Generation from seed: ll just se\n",
      "ll just see the mock turtle that was the mock turtle that was the mock turtle that was the mock turtle that wa==================================================\n",
      "Iteration #: 20\n",
      "Generation from seed: , it is hi\n",
      ", it is his hatter as it was a little good she had not a contering about it was go and dont make again, and sh==================================================\n",
      "Iteration #: 21\n",
      "Generation from seed: other end \n",
      "other end of the court any distly sore hand any mister with the caterpillar said the mock turtle that the mous==================================================\n",
      "Iteration #: 22\n",
      "Generation from seed: d take the\n",
      "d take the remerself and the rabbit would not a sight was a little with the gryphon sittly the project gutenbe==================================================\n",
      "Iteration #: 23\n",
      "Generation from seed: ht, what a\n",
      "ht, what a great dont know what was soon a long the dormouse for she had not could be the court a should the d==================================================\n",
      "Iteration #: 24\n",
      "Generation from seed: said alice\n",
      "said alice. i should heard the king and the king and the king and the king and the king and the king and the k\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\"*50)\n",
    "    print(\"Iteration #: %d\" % (iteration))\n",
    "    model.fit(X,Y,batch_size=BATCH_SIZE, epochs=NUM_EPOCH_PER_ITERATION, verbose=0)\n",
    "    \n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "    print(\"Generation from seed: %s\" % (test_chars))\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for i, ch in enumerate(test_chars):\n",
    "            Xtest[0, i, char2index[ch]] = 1\n",
    "        pred = model.predict(Xtest, verbose=0)[0]\n",
    "        ypred = index2char[np.argmax(pred)]\n",
    "        print(ypred, end=\"\")\n",
    "        # move forward with test_chars + ypred\n",
    "        test_chars = test_chars[1:] + ypred\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
