{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU - POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A POS é uma categoria gramatical de palavras que são usadas da mesma forma através de multiplex frases. Exemplo de POS tagging são nomes, verbos, adjetivos, etc. Por exemplo, nomes são tipicamente utilizados para identificar coisas, verbos são tipicamente utilizados para identificar o que esta fazendo, e adjetivos para descrever alguns atributos das coisas. POS tagging normalmente é realizado manualmente, mais atualmente é feito automaticamente utilizando modelos estatísticos. Nos últimos anos Deep Learning tem sido aplicado nestes problemas com bons resultados (para maior informação ler artigo: [Natural Language Processing (almost) from Scratch](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf) , R. Collobert, Journal of Machine Learning Research. 2011).\n",
    "\n",
    "Para nossos dados de treinamento, precisamos de frases marcadas com parte das tags da fala. O dataset [The Peen Treebank](https://catalog.ldc.upenn.edu/ldc99t42), é uma anotação de 4.5 milhões de palavras de Inglês Americano. A data não é livre, um 10% do dataset é disponível em NLTK (http://www.nltk.org/), que utilizaremos em nossa rede.\n",
    "\n",
    "O modelo pegara uma seqüência de palavras em uma frase e predicará o correspondente POS tags para cada palavra. Assim, para uma seqüência contida pelas palavras [The, cat, sat, on, the, mat.], a seqüência de saída é [DT, NN, VB, IN, DT, NN]. Ref: [https://cs.nyu.edu/grishman/jet/guide/PennPOS.html](https://cs.nyu.edu/grishman/jet/guide/PennPOS.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla C2070 (CNMeM is enabled with initial size: 95.0% of memory, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixamos os dados desde o NLTK num formato. Utilizamos o seguinte código em Python para baixar os dados em 2 arquivos paralelos, um para as palavras nas orações, e outro para os POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_DIR=\"data\"\n",
    "\n",
    "fedata = open(os.path.join(DATA_DIR, \"treebank_sents.txt\"),\"w\")\n",
    "ffdata = open(os.path.join(DATA_DIR, \"treebank_poss.txt\"), \"w\")\n",
    "\n",
    "sents = nltk.corpus.treebank.tagged_sents()\n",
    "for sent in sents:\n",
    "    words, poss = [], []\n",
    "    for word,pos in sent:\n",
    "        if pos == \"-NONE-\":\n",
    "            continue\n",
    "        words.append(word)\n",
    "        poss.append(pos)\n",
    "    fedata.write(\"{}\\n\".format(\" \".join(words)))\n",
    "    ffdata.write(\"{}\\n\".format(\" \".join(poss)))\n",
    "    \n",
    "fedata.close()\n",
    "ffdata.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Word Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos explorar os dados um pouco para encontrar o tamanho de vocabulário a utilizar. Neste caso, temos que considerar 2 tipos diferentes de vocabulários, a fonte de vocabulários para as palavras e para os tags. Precisamos encontrar um número único de palavras em cada vocabulário, também precisamos encontrar o número máximo de palavras em uma frase em nosso dataset de treinamento e o número de instancias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10947 249 3914 45 249 3914\n"
     ]
    }
   ],
   "source": [
    "def parse_sentences(filename):\n",
    "    word_freqs = collections.Counter()\n",
    "    num_recs, maxlen = 0, 0\n",
    "    fin = open(filename, 'rb')\n",
    "    for line in fin:\n",
    "        words = line.strip().lower().split()\n",
    "        for word in words:\n",
    "            word_freqs[word] += 1\n",
    "        if len(words) > maxlen:\n",
    "            maxlen = len(words)\n",
    "        num_recs += 1\n",
    "    fin.close()\n",
    "    return word_freqs, maxlen, num_recs\n",
    "\n",
    "s_wordfreqs, s_maxlen, s_numrecs = parse_sentences(os.path.join(DATA_DIR, \"treebank_sents.txt\"))\n",
    "t_wordfreqs, t_maxlen, t_numrecs = parse_sentences(os.path.join(DATA_DIR, \"treebank_poss.txt\"))\n",
    "print(len(s_wordfreqs), s_maxlen, s_numrecs, len(t_wordfreqs), t_maxlen, t_numrecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Número de palavras únicas: **10947** <br/>\n",
    "Número de POS tags únicos: **45** <br/>\n",
    "Número máximo de palavras por frase : **249**<br/>\n",
    "Número de frases (10%) : **3914**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Lookup Table\n",
    "\n",
    "Cada entrada é representada por uma sequencia de índices das palavras. A saída é a sequencia de índices dos POS tags. Então, precisamos construir **lookup table** para transformar entre as palavras/POS tags e seu índice correspondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQLEN = 250\n",
    "S_MAX_FEATURES = 5000\n",
    "T_MAX_FEATURES = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_vocabsize = min(len(s_wordfreqs), S_MAX_FEATURES) + 2\n",
    "s_word2index = {x[0]: i+2 for i, x in enumerate(s_wordfreqs.most_common(S_MAX_FEATURES))}\n",
    "s_word2index[\"PAD\"] = 0\n",
    "s_word2index[\"UNK\"] = 1\n",
    "s_index2word = {v:k for k, v in s_word2index.items()}\n",
    "\n",
    "t_vocabsize = len(t_wordfreqs) + 1\n",
    "t_word2index = {x[0]:i for i, x in enumerate(t_wordfreqs.most_common(T_MAX_FEATURES))}\n",
    "t_word2index[\"PAD\"] = 0\n",
    "t_index2word = {v:k for k, v in t_word2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create input and output dataset for train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O seguinte passo é construir nosso dataset para a nossa rede. Utilizaremos os **Lookup table** para converter as frases em ID de sequencias com dimensão MAX_SEQLEN (250). Os rótulos precisam ser estruturadas como uma sequencia de **one-hot** vectors de dimensão T_MAX_FEATURES + 1 (46), também de dimensão MAX_SEQLEN (250). A função ```build_tensor``` lê os dados dos dois arquivos e converte para os tensores de entradas e saídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_tensor(filename, numrecs, word2index, maxlen, make_categorical=False, num_classes=0):\n",
    "    data = np.empty((numrecs,), dtype=list)\n",
    "    fin = open(filename, \"rb\")\n",
    "    i = 0\n",
    "    for line in fin:\n",
    "        wids = []\n",
    "        for word in line.strip().lower().split():\n",
    "            if word in word2index:\n",
    "                wids.append(word2index[word])\n",
    "            else:\n",
    "                wids.append(word2index[\"UNK\"])\n",
    "        if make_categorical:\n",
    "            data[i] = np_utils.to_categorical(wids, num_classes=num_classes) #[[[01000],[10000],[0001]]\n",
    "        else:\n",
    "            data[i] = wids #[[8,16,1]]\n",
    "        i += 1\n",
    "    fin.close()\n",
    "    pdata = sequence.pad_sequences(data, maxlen=maxlen)\n",
    "    return pdata\n",
    "\n",
    "X = build_tensor(os.path.join(DATA_DIR, \"treebank_sents.txt\"), s_numrecs, s_word2index, MAX_SEQLEN)\n",
    "Y = build_tensor(os.path.join(DATA_DIR, \"treebank_poss.txt\"), t_numrecs, t_word2index, MAX_SEQLEN, \n",
    "                 True, t_vocabsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, dividimos nossos dados de treinamento e teste (80-20):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y, test_size=0.2, random_state=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
