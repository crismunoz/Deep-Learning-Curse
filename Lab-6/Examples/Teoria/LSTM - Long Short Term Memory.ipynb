{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long short term memory - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM é uma variante de RNN que tem a capacidade de aprender dependências ao longo prazo. LSTMs foi proposto inicialmente por Hochreiter e Schmidhuber e foi redefinida por vários pesquisadores. Eles trabalham bem numa grande variedade de problemas e são o tipo de RNNs mais utilizada.\n",
    "\n",
    "Temos visto como uma SimpleRNN utiliza um estado oculto do estado anterior e a actual entrada numa camada tanh para implementar a recorrência. LSTM também implementa recorrência de uma forma similar, mas invés de utilizar uma única camada tanh existe 4 camadas interactuando de uma forma bem especifica. O seguinte diagrama apresenta a transformações que são aplicadas ao estado oculto no tempo t:\n",
    "\n",
    "![](img/lstm.png)\n",
    "\n",
    "O diagrama parece complicado, mas observemos componente por componente. A linha na parte inferior do diagrama é o estado oculto, e *i,f,o* e *g* são os mecanismos pelo qual LSTM consegue resolver o problema de desaparecimento do gradiente. Durante o treinamento, a LSTM aprende os parâmetros por estas portas.\n",
    "\n",
    "Para ter um entendimento melhor de como estas portas modulam o estado oculto do LSTM, consideramos a equação que mostra como ele calcula o estado oculto $h_t$ no tempo t desde o estado oculto $h_{t-1} do tempo anterior.\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "i &=& \\sigma(W_{i}h_{t} + U_{i}x_{t}) \\\\\n",
    "f &=& \\sigma(W_{f}h_{t-1} + U_{f}x_{t}) \\\\\n",
    "o &=& \\sigma(W_{o}h_{t-1} + U_{o}x_{t}) \\\\\n",
    "g &=& tanh(W_{g}h_{t-1} + U_{g}x_{t}) \\\\\n",
    "c_{t} &=& (c_{t-1} \\otimes f) \\otimes (g \\otimes i) \\\\\n",
    "h_{t} &=& tanh(c_{t}) \\otimes 0\n",
    "\\end{eqnarray}\n",
    "\n",
    "Onde, $i$, $f$, e $o$ são as portas de entrada, esquecimento e saída respectivamente. Elas são calculadas utilizando as mesmas equações mas com diferentes matrizes de parâmetros. A função sigmoidal modula a saída de estas portas entre 0 e 1. Assim, o vector de saída produzida pode ser multiplicado ponto a ponto por outro vector para definir quanto do segundo vector pode passar através do primeiro.\n",
    "\n",
    "A porta de esquecimento, define quanto do estado prévio $h_{t-1}$ deseja-se passar. A porta de entrada define quanto do novo estado calculado da actual entrada $x_t$ e do estado prévio $h_{t-1}$. Observe que a equação para g é idêntica que de uma célula SimpleRNN, mais neste caso modulamos a saída com a saída da porta de entrada $i$.\n",
    "\n",
    "Dado $i$, $f$, $o$ e $g$, podem calcular as células de estado $c_{t}$ no tempo $t$ em termo de $c_{t-1}$ no tempo $(t-1)$ multiplicado com a porta de esquecimento e o estado $g$ multiplicado pela porta de entrada $i$. Então, esta é a forma básica de combinar a memoria previa com a nova entrada, configurando a porta de esquecimento a 0, ignora a memoria anterior e configura a porta de entrada a 0, ignora os novos estados computados.\n",
    "\n",
    "Finalmente, a camada oculta $h_{t}$ no tempo t é calculado multiplicando a memoria $c_{t}$ com a porta de saida.\n",
    "\n",
    "Uma coisa a perceber é que uma LSTM é uma substituição de uma célula simples RNN, a única diferença é que LSTMs são resistentes ao problema de desaparecimento do gradiente. Você pode substituir uma célula RNN em uma rede com um LSTM sem se preocupar com quaisquer efeitos colaterais. Você geralmente deve ver melhores resultados, juntamente com um maior tempo de treinamento.\n",
    "\n",
    "Referencias: \n",
    "* [Undestanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Deep Learning with Keras](http:// colah.github.io/ posts/ 2015-08-Understanding-LSTMs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
