{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:768:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4185:(_snd_config_evaluate) function snd_func_card_driver returned error: Permission denied\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4185:(_snd_config_evaluate) function snd_func_concat returned error: Permission denied\n",
      "ALSA lib confmisc.c:1251:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4185:(_snd_config_evaluate) function snd_func_refer returned error: Permission denied\n",
      "ALSA lib conf.c:4664:(snd_config_expand) Evaluate error: Permission denied\n",
      "ALSA lib pcm.c:2209:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"game.py\", line 122, in <module>\n",
      "    game.play()\n",
      "  File \"game.py\", line 102, in play\n",
      "    pygame.display.flip()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python game.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division, print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from scipy.misc import imresize\n",
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import wrapped_game\n",
    "\n",
    "def preprocess_images(images):\n",
    "    if images.shape[0] < 4:\n",
    "        # single image\n",
    "        x_t = images[0]\n",
    "        x_t = imresize(x_t, (80, 80))\n",
    "        x_t = x_t.astype(\"float\")\n",
    "        x_t /= 255.0\n",
    "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    else:\n",
    "        # 4 images\n",
    "        xt_list = []\n",
    "        for i in range(images.shape[0]):\n",
    "            x_t = imresize(images[i], (80, 80))\n",
    "            x_t = x_t.astype(\"float\")\n",
    "            x_t /= 255.0\n",
    "            xt_list.append(x_t)\n",
    "        s_t = np.stack((xt_list[0], xt_list[1], xt_list[2], xt_list[3]), \n",
    "                       axis=2)\n",
    "    s_t = np.expand_dims(s_t, axis=0)\n",
    "    return s_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_batch(experience, model, num_actions, gamma, batch_size):\n",
    "    batch_indices = np.random.randint(low=0, high=len(experience),\n",
    "                                      size=batch_size)\n",
    "    batch = [experience[i] for i in batch_indices]\n",
    "    X = np.zeros((batch_size, 80, 80, 4))\n",
    "    Y = np.zeros((batch_size, num_actions))\n",
    "    for i in range(len(batch)):\n",
    "        s_t, a_t, r_t, s_tp1, game_over = batch[i]\n",
    "        X[i] = s_t\n",
    "        Y[i] = model.predict(s_t)[0]\n",
    "        Q_sa = np.max(model.predict(s_tp1)[0])\n",
    "        if game_over:\n",
    "            Y[i, a_t] = r_t\n",
    "        else:\n",
    "            Y[i, a_t] = r_t + gamma * Q_sa\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################# main ###############################\n",
    "# initialize parameters\n",
    "DATA_DIR = \"data\"\n",
    "NUM_ACTIONS = 3 # number of valid actions (left, stay, right)\n",
    "GAMMA = 0.99 # decay rate of past observations\n",
    "INITIAL_EPSILON = 0.1 # starting value of epsilon\n",
    "FINAL_EPSILON = 0.0001 # final value of epsilon\n",
    "MEMORY_SIZE = 50000 # number of previous transitions to remember\n",
    "NUM_EPOCHS_OBSERVE = 100\n",
    "NUM_EPOCHS_TRAIN = 2000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = NUM_EPOCHS_OBSERVE + NUM_EPOCHS_TRAIN\n",
    "\n",
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=8, strides=4, \n",
    "                 kernel_initializer=\"normal\", \n",
    "                 padding=\"same\",\n",
    "                 input_shape=(80, 80, 4)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(64, kernel_size=4, strides=2, \n",
    "                 kernel_initializer=\"normal\", \n",
    "                 padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Conv2D(64, kernel_size=3, strides=1,\n",
    "                 kernel_initializer=\"normal\",\n",
    "                 padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, kernel_initializer=\"normal\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(3, kernel_initializer=\"normal\"))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=1e-6), loss=\"mse\")\n",
    "\n",
    "# train network\n",
    "game = wrapped_game.MyWrappedGame()\n",
    "experience = collections.deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "fout = open(os.path.join(DATA_DIR, \"rl-network-results.tsv\"), \"wb\")\n",
    "num_games, num_wins = 0, 0\n",
    "epsilon = INITIAL_EPSILON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001/2100 | Loss 0.00000 | Win Count: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c7dead4ca3ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m           .format(e + 1, NUM_EPOCHS, loss, num_wins))\n\u001b[1;32m     44\u001b[0m     fout.write(\"{:04d}\\t{:.5f}\\t{:d}\\n\"\n\u001b[0;32m---> 45\u001b[0;31m           .format(e + 1, loss, num_wins))\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "for e in range(NUM_EPOCHS):\n",
    "    loss = 0.0\n",
    "    game.reset()\n",
    "    \n",
    "    # get first state\n",
    "    a_0 = 1  # (0 = left, 1 = stay, 2 = right)\n",
    "    x_t, r_0, game_over = game.step(a_0) \n",
    "    s_t = preprocess_images(x_t)\n",
    "\n",
    "    while not game_over:\n",
    "        s_tm1 = s_t\n",
    "        # next action\n",
    "        if e <= NUM_EPOCHS_OBSERVE:\n",
    "            a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n",
    "        else:\n",
    "            if np.random.rand() <= epsilon:\n",
    "                a_t = np.random.randint(low=0, high=NUM_ACTIONS, size=1)[0]\n",
    "            else:\n",
    "                q = model.predict(s_t)[0]\n",
    "                a_t = np.argmax(q)\n",
    "                \n",
    "        # apply action, get reward\n",
    "        x_t, r_t, game_over = game.step(a_t)\n",
    "        s_t = preprocess_images(x_t)\n",
    "        # if reward, increment num_wins\n",
    "        if r_t == 1:\n",
    "            num_wins += 1\n",
    "        # store experience\n",
    "        experience.append((s_tm1, a_t, r_t, s_t, game_over))\n",
    "        \n",
    "        if e > NUM_EPOCHS_OBSERVE:\n",
    "            # finished observing, now start training\n",
    "            # get next batch\n",
    "            X, Y = get_next_batch(experience, model, NUM_ACTIONS, \n",
    "                                  GAMMA, BATCH_SIZE)\n",
    "            loss += model.train_on_batch(X, Y)\n",
    "        \n",
    "    # reduce epsilon gradually\n",
    "    if epsilon > FINAL_EPSILON:\n",
    "        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / NUM_EPOCHS\n",
    "        \n",
    "    print(\"Epoch {:04d}/{:d} | Loss {:.5f} | Win Count: {:d}\"\n",
    "          .format(e + 1, NUM_EPOCHS, loss, num_wins))\n",
    "    fout.write(\"{:04d}\\t{:.5f}\\t{:d}\\n\".format(str(e + 1), str(loss), str(num_wins)))\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        model.save(os.path.join(DATA_DIR, \"rl-network.h5\"), overwrite=True)\n",
    "        \n",
    "fout.close()\n",
    "model.save(os.path.join(DATA_DIR, \"rl-network.h5\"), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
